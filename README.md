<h2>**Initial Setup & Model Loading:**</h2>

**Environment Setup:** 
Necessary libraries (accelerate, transformers, nltk, rouge-score) are installed, and NLTK data is downloaded.

**Model Loading:**
The Salesforce/instructblip-flan-t5-xl VLM and its processor are loaded. This model is chosen for its instruction-following capabilities and its ability to fit on a Tesla P100 (16GB VRAM) GPU in torch.float16 (half-precision) without quantization. This ensures optimal performance while respecting memory constraints.

<h3>Data & Prompting</h3>

**Data Loading:** 
The system loads a data.json file containing existing recipe data (image filenames, true titles, and manual summaries) along with a corresponding images folder. This data.json serves as the source for our few-shot examples and initial ground truth.

**Few-Shot Prompting (Text-Only):** 
For each new test image, the VLM is given a carefully constructed prompt that includes the current image query and three randomly selected text-only few-shot examples from data.json. These examples demonstrate the desired output format (a numbered list of concise steps), teaching the model in-context how to structure its responses. While the model processes the image visually, the few-shot examples themselves are provided as text to manage GPU memory limitations.

_**Crucially, every summary generated by the model (both few-shot and zero-shot) is integrated back into data.json for future use. This means that over time, the model will benefit from a growing dataset of generated summaries, allowing for gradual, self-improving few-shot prompting as the dataset expands. For the initial phase, all few-shot examples are based on the manually generated summaries in data.json.**_

Zero-Shot Prompting: For comparison, each test image is also processed with a simple prompt that includes only the image query, without any few-shot examples.
