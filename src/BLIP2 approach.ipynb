{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11929638,"sourceType":"datasetVersion","datasetId":7500031},{"sourceId":11942085,"sourceType":"datasetVersion","datasetId":7507446},{"sourceId":11966896,"sourceType":"datasetVersion","datasetId":7524950},{"sourceId":242135712,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport random\nimport json\nimport torch\nfrom PIL import Image\nfrom transformers import Blip2ForConditionalGeneration, AutoProcessor # InstructBLIP uses Blip2ForConditionalGeneration class\nimport pandas as pd\nimport accelerate\nimport transformers\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:53:03.949867Z","iopub.execute_input":"2025-05-27T16:53:03.950591Z","iopub.status.idle":"2025-05-27T16:53:29.074740Z","shell.execute_reply.started":"2025-05-27T16:53:03.950563Z","shell.execute_reply":"2025-05-27T16:53:29.073874Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 16:53:17.650144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748364797.838025      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748364797.901948      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/images/stacked circle feast.png\n/kaggle/input/images/cheesy bake mess.png\n/kaggle/input/images/green swirl delight.png\n/kaggle/input/images/white puff plate.png\n/kaggle/input/images/melty crunch.png\n/kaggle/input/images/soupy beans.png\n/kaggle/input/images/yellow spirals.png\n/kaggle/input/images/goey dark bomb.png\n/kaggle/input/images/bready tubes.png\n/kaggle/input/images/choco fluff.png\n/kaggle/input/images/stackysyrupthing.png\n/kaggle/input/images/bun crunch.png\n/kaggle/input/data-json/data.json\n/kaggle/input/test-set/butter potato.png\n/kaggle/input/test-set/stuffed rice folded.png\n/kaggle/input/test-set/crunchy yellow sticks.png\n/kaggle/input/test-set/sticky sweet swirl.png\n/kaggle/input/test-set/spicey cruncy chinese.png\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"json_path = \"/kaggle/input/data-json/data.json\" # Path to your few-shot data\nfew_shot_image_folder = \"/kaggle/input/images\"  # Folder for few-shot example images\ntest_image_folder = \"/kaggle/input/test-set\"    # Folder for NEW test set images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:53:29.075495Z","iopub.execute_input":"2025-05-27T16:53:29.076032Z","iopub.status.idle":"2025-05-27T16:53:29.079641Z","shell.execute_reply.started":"2025-05-27T16:53:29.076011Z","shell.execute_reply":"2025-05-27T16:53:29.079065Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_id = \"Salesforce/instructblip-flan-t5-xl\"\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16, \n)\nprocessor = AutoProcessor.from_pretrained(model_id)\nprint(\"Model loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:53:29.081192Z","iopub.execute_input":"2025-05-27T16:53:29.081909Z","iopub.status.idle":"2025-05-27T16:55:07.815878Z","shell.execute_reply.started":"2025-05-27T16:53:29.081881Z","shell.execute_reply":"2025-05-27T16:55:07.814330Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8345f3699ad4f79b6df0dae0f577184"}},"metadata":{}},{"name":"stderr","text":"You are using a model of type instructblip to instantiate a model of type blip-2. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/135k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307873544c4e4fc88f98dede9b950c4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed1fffe4acc49aa97258281c2029daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860f4f1ec9cb4f72a232db7370c65201"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/6.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25693e9825214ceeaa0d68808c3a6cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bca513d3e1347ada570ff864cae385b"}},"metadata":{}},{"name":"stderr","text":"Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at Salesforce/instructblip-flan-t5-xl and are newly initialized: ['qformer.layernorm.bias', 'qformer.layernorm.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8496cdc312471bb618fff21adaeba6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/75.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931c1d66c7af451180904d0180e6514f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7adfde0cbb4c898a35eec845e3d4f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76713fe1deff4b20b1081a05a2f57ae4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a98c99a8c74714ad74cae43f610f39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7611f89ca67f4f5a8ead1f49ab4ac2ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a41bdf283dc451d88db08e816b753c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c93f3f7464fa464a9fb9955da64e6c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d348c75909674e68ac2b1c16a7ddda91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79618ac73544a3d9d01659074645a69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079f95a6692b45759a34b5337d2e3f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc5fcf6bf2864c35b838c5a533af16b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/833 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12408882d6543e6bdd190e6b7747ac3"}},"metadata":{}},{"name":"stdout","text":"Model loaded\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"few_shot_data = []\ntry:\n    with open(json_path, \"r\") as f:\n        few_shot_data = json.load(f)\n    print(f\"Loaded {len(few_shot_data)} recipes from {json_path} for few-shot examples.\")\nexcept FileNotFoundError:\n    print(f\"Error: JSON file not found at {json_path}. Please ensure the path is correct.\")\n    exit()\n\ntest_data = [\n    {\n        \"image\": \"butter potato.png\",\n        \"true_title\": \"Butter Potato\",\n        \"manual_summary\": [\n            \"Bake potatoes until soft.\",\n            \"Cut them open, fluff with a fork, and add butter and seasonings.\",\n            \"Serve hot with desired toppings.\"\n        ]\n    },\n    {\n        \"image\": \"crunchy yellow sticks.png\",\n        \"true_title\": \"Crunchy Yellow Sticks\",\n        \"manual_summary\": [\n            \"Cut potatoes into thin sticks and soak in cold water.\",\n            \"Deep-fry in hot oil until golden brown and crispy.\",\n            \"Season with salt and serve immediately with ketchup.\"\n        ]\n    },\n    {\n        \"image\": \"spicey cruncy chinese.png\",\n        \"true_title\": \"Spicy Crunchy Chinese\",\n        \"manual_summary\": [\n            \"Deep-fry chicken pieces until crisp.\",\n            \"Sauté with ginger, garlic, and chilies in a tangy, spicy sauce.\",\n            \"Garnish with spring onions and serve hot.\"\n        ]\n    },\n    {\n        \"image\": \"sticky sweet swirl.png\",\n        \"true_title\": \"Sticky Sweet Swirl\",\n        \"manual_summary\": [\n            \"Prepare a soft dough and roll it flat.\",\n            \"Spread with cinnamon-sugar filling and roll tightly, then slice.\",\n            \"Bake until golden and drizzle with a sweet glaze.\"\n        ]\n    },\n    {\n        \"image\": \"stuffed rice folded.png\",\n        \"true_title\": \"Stuffed Rice Folded\",\n        \"manual_summary\": [\n            \"Prepare a savory minced meat or vegetable filling.\",\n            \"Enclose the filling in thin dough wrappers, shaping into pockets.\",\n            \"Steam until fully cooked and serve with a spicy dipping sauce.\"\n        ]\n    }\n]\nif not few_shot_data:\n    print(\"No few-shot data\")\n    exit()\nif not test_data:\n    print(\"No test data \")\n    exit()\n\nprint(f\"Using {len(few_shot_data)} data.json few-shot examples.\")\nprint(f\"Using {len(test_data)} testing (from {test_image_folder}).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:55:07.817011Z","iopub.execute_input":"2025-05-27T16:55:07.817350Z","iopub.status.idle":"2025-05-27T16:55:07.832177Z","shell.execute_reply.started":"2025-05-27T16:55:07.817313Z","shell.execute_reply":"2025-05-27T16:55:07.831116Z"}},"outputs":[{"name":"stdout","text":"Loaded 12 recipes from /kaggle/input/data-json/data.json for few-shot examples.\nUsing 12 data.json few-shot examples.\nUsing 5 testing (from /kaggle/input/test-set).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def summary_few_shot(image_path, dish_title, model, processor, few_shot_examples_data, total_few_shot_examples_in_prompt=3):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n    except FileNotFoundError:\n        print(f\"Warning: Test image not found at {image_path}. Returning 'Image not found.'\")\n        return \"Image not found.\"\n    selected_few_shot_examples = random.sample(few_shot_examples_data, min(total_few_shot_examples_in_prompt, len(few_shot_examples_data)))\n    full_prompt = \"\"\n    for example in selected_few_shot_examples:\n        example_summary_str = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(example['manual_summary'])])\n        full_prompt += (\n            f\"Question: Describe the exact cooking steps for the dish '{example['true_title']}'. \"\n            f\"Provide precisely 3, short, imperative sentences, formatted as a numbered list. \"\n            f\"Start directly with '1. '. Do not include any introductory phrases, concluding remarks, or conversational text. \"\n            f\"Each step should be a single, action-oriented sentence.\\n\"\n            f\"Answer:\\n{example_summary_str}\\n\"\n        )\n    full_prompt += (\n        f\"Question: Describe the exact cooking steps for the dish '{dish_title}' shown in the image. \"\n        f\"Provide precisely 3, short, imperative sentences, formatted as a numbered list. \"\n        f\"Start directly with '1. '. Do not include any introductory phrases, concluding remarks, or conversational text. \"\n        f\"Each step should be a single, action-oriented sentence.\\n\"\n        f\"Answer:\\n\"\n    )\n    inputs = processor(images=image, text=full_prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(\n        pixel_values=inputs.pixel_values,\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=80,\n        num_beams=5,\n        do_sample=False,\n        early_stopping=True\n    )\n    generated_text = processor.decode(output[0], skip_special_tokens=True).strip()\n#output was sometimes returning multiple when I varied temp and params. it is (list)\n    if \"Answer:\" in generated_text:\n        generated_text = generated_text.rsplit(\"Answer:\", 1)[-1].strip()\n    if \"Question:\" in generated_text:\n        generated_text = generated_text.rsplit(\"Question:\", 1)[-1].strip()\n    lines = generated_text.split('\\n')\n    filtered_lines = [line.strip() for line in lines if line.strip() and (line.strip().startswith(tuple(str(i) + '.' for i in range(1, 4))) or len(line.strip()) > 5)]\n    parsed_summary = \"\\n\".join(filtered_lines[:3]) if filtered_lines else generated_text\n\n    return parsed_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:55:07.833381Z","iopub.execute_input":"2025-05-27T16:55:07.833778Z","iopub.status.idle":"2025-05-27T16:55:07.861975Z","shell.execute_reply.started":"2025-05-27T16:55:07.833752Z","shell.execute_reply":"2025-05-27T16:55:07.861053Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def summary_zero_shot(image_path, dish_title, model, processor):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n    except FileNotFoundError:\n        print(f\"Warning: Image not found at {image_path}. Returning 'Image not found.'\")\n        return \"Image not found.\"\n    full_prompt = (\n        f\"Question: Describe the exact cooking steps for the dish '{dish_title}' shown in the image. \"\n        f\"Provide precisely 3, short, imperative sentences, formatted as a numbered list. \"\n        f\"Start directly with '1. '. Do not include any introductory phrases, concluding remarks, or conversational text. \"\n        f\"Each step should be a single, action-oriented sentence.\\n\"\n        f\"Answer:\\n\"\n    )\n\n    inputs = processor(images=image, text=full_prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(\n        pixel_values=inputs.pixel_values,\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask ,#ye default values hai, do not change to ** input gives model_kwargs error\n        max_new_tokens=100,\n        num_beams=5,\n        do_sample=False,\n        early_stopping=True\n    )\n    generated_text = processor.decode(output[0], skip_special_tokens=True).strip()\n    if \"Answer:\" in generated_text:\n        generated_text = generated_text.rsplit(\"Answer:\", 1)[-1].strip()\n    if \"Question:\" in generated_text:\n        generated_text = generated_text.rsplit(\"Question:\", 1)[-1].strip()\n    lines = generated_text.split('\\n')\n    # Filter for lines that start with a number (1., 2., 3.) or have some content\n    filtered_lines = [line.strip() for line in lines if line.strip() and (line.strip().startswith(tuple(str(i) + '.' for i in range(1, 4))) or len(line.strip()) > 5)]\n    parsed_summary = \"\\n\".join(filtered_lines[:3]) if filtered_lines else generated_text\n\n    return parsed_summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:55:07.862743Z","iopub.execute_input":"2025-05-27T16:55:07.863095Z","iopub.status.idle":"2025-05-27T16:55:07.881999Z","shell.execute_reply.started":"2025-05-27T16:55:07.863062Z","shell.execute_reply":"2025-05-27T16:55:07.881314Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"few_shot_results = []\nprint(\"Generating cooking summaries (Few-Shot Inference on New Test Set):\")\nTOTAL_TEXT_FEW_SHOT_COUNT = 3 \n\nfor item in test_data:\n    title = item.get(\"true_title\", \"Unknown\")\n    image_filename = item[\"image\"]\n    image_path = os.path.join(test_image_folder, image_filename)\n    ground_truth_summary = \"\\n\".join(item[\"manual_summary\"])\n    print(f\" Processing: {title} ({image_filename})\")\n    instructblip_summary = summary_few_shot( \n        image_path,\n        title,\n        model,\n        processor,\n        few_shot_data, # Pass all few-shot examples for random selection\n        total_few_shot_examples_in_prompt=TOTAL_TEXT_FEW_SHOT_COUNT\n    )\n    few_shot_results.append({\n        \"title\": title,\n        \"image\": image_filename,\n        \"ground_truth_summary\": ground_truth_summary,\n        \"instructblip_summary\": instructblip_summary \n    })\n\n    print(\"Ground Truth:\", ground_truth_summary)\n    print(\"Generated Summary:\" ,instructblip_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:55:07.882812Z","iopub.execute_input":"2025-05-27T16:55:07.883077Z","iopub.status.idle":"2025-05-27T17:14:52.252597Z","shell.execute_reply.started":"2025-05-27T16:55:07.883052Z","shell.execute_reply":"2025-05-27T17:14:52.251658Z"}},"outputs":[{"name":"stdout","text":"Generating cooking summaries (Few-Shot Inference on New Test Set):\n Processing: Butter Potato (butter potato.png)\nGround Truth: Bake potatoes until soft.\nCut them open, fluff with a fork, and add butter and seasonings.\nServe hot with desired toppings.\nGenerated Summary: 1. Preheat oven to 375 degrees F. Line a baking sheet with parchment paper. 2. In a large bowl, combine butter, flour, salt, pepper, and paprika. 3. In a separate bowl, whisk together eggs, milk, buttermilk, and sour cream. 4. Pour the wet ingredients into the dry ingredients and stir with a wooden spoon\n Processing: Crunchy Yellow Sticks (crunchy yellow sticks.png)\nGround Truth: Cut potatoes into thin sticks and soak in cold water.\nDeep-fry in hot oil until golden brown and crispy.\nSeason with salt and serve immediately with ketchup.\nGenerated Summary: 1. Heat oil in a large frying pan over medium heat. Add onion and garlic and cook for 2 minutes. 2. Add tomatoes and cook for 2 minutes. 3. Add spices and cook for 2 minutes. 4. Remove from heat.\n Processing: Spicy Crunchy Chinese (spicey cruncy chinese.png)\nGround Truth: Deep-fry chicken pieces until crisp.\nSauté with ginger, garlic, and chilies in a tangy, spicy sauce.\nGarnish with spring onions and serve hot.\nGenerated Summary: 1. Heat oil in a large frying pan over medium heat. Add onion and garlic and cook until softened, about 5 minutes. Add garlic and ginger and cook until fragrant, about 30 seconds. 2. Add beef and cook until tender, about 5 minutes. 3. Add carrots and cook until tender, about 5 minutes. 4. Add mushrooms and cook until tender, about 5 minutes. 5. Add\n Processing: Sticky Sweet Swirl (sticky sweet swirl.png)\nGround Truth: Prepare a soft dough and roll it flat.\nSpread with cinnamon-sugar filling and roll tightly, then slice.\nBake until golden and drizzle with a sweet glaze.\nGenerated Summary: 1. Preheat the oven to 350 degrees f (180 degrees celsius). Line a baking sheet with parchment paper. 2. In a large bowl, combine the flour, sugar, baking powder, baking soda, cinnamon, nutmeg, baking powder, baking soda, baking powder, baking soda, cinnamon, nutmeg, baking soda, baking powder, baking soda\n Processing: Stuffed Rice Folded (stuffed rice folded.png)\nGround Truth: Prepare a savory minced meat or vegetable filling.\nEnclose the filling in thin dough wrappers, shaping into pockets.\nSteam until fully cooked and serve with a spicy dipping sauce.\nGenerated Summary: Stuffed Rice Folded\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"zero_shot_results = []\nprint(\"Generating cooking summaries (Zero-Shot Inference on New Test Set):\")\n\nfor item in test_data:\n    title = item.get(\"true_title\", \"Unknown\")\n    image_filename = item[\"image\"]\n    image_path = os.path.join(test_image_folder, image_filename)\n    ground_truth_summary = \"\\n\".join(item[\"manual_summary\"])\n\n    print(f\" Processing: {title} ({image_filename})\")\n    instructblip_summary = summary_zero_shot(\n        image_path,\n        title,\n        model,\n        processor\n    )\n\n    zero_shot_results.append({\n        \"title\": title,\n        \"image\": image_filename,\n        \"ground_truth_summary\": ground_truth_summary,\n        \"instructblip_summary\": instructblip_summary \n    })\n\n    print(\"Ground Truth:\",ground_truth_summary)\n    print(\"Generated Summary:\", instructblip_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T17:14:52.253731Z","iopub.execute_input":"2025-05-27T17:14:52.254002Z","execution_failed":"2025-05-27T17:20:22.332Z"}},"outputs":[{"name":"stdout","text":"Generating cooking summaries (Zero-Shot Inference on New Test Set):\n Processing: Butter Potato (butter potato.png)\nGround Truth: Bake potatoes until soft.\nCut them open, fluff with a fork, and add butter and seasonings.\nServe hot with desired toppings.\nGenerated Summary: a person is preparing a dish of mashed potatoes\n Processing: Crunchy Yellow Sticks (crunchy yellow sticks.png)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install -U accelerate transformers nltk rouge-score\nimport nltk\ntry:\n    nltk.data.find('punkt')\nexcept nltk.downloader.DownloadError:\n    nltk.download('punkt')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-27T17:20:22.332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# from rouge_score import rouge_scorer\n\n# # --- BLEU and ROUGE Score Calculation ---\n# print(\"\\n\" + \"=\"*50)\n# print(\"Calculating BLEU and ROUGE Scores:\")\n# print(\"=\"*50 + \"\\n\")\n\n# # Initialize ROUGE scorer\n# scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# # Prepare data for scoring\n# few_shot_references = [nltk.word_tokenize(item['ground_truth_summary'].lower()) for item in few_shot_results]\n# few_shot_hypotheses = [nltk.word_tokenize(item['instructblip_summary'].lower()) for item in few_shot_results]\n\n# zero_shot_references = [nltk.word_tokenize(item['ground_truth_summary'].lower()) for item in zero_shot_results]\n# zero_shot_hypotheses = [nltk.word_tokenize(item['instructblip_summary'].lower()) for item in zero_shot_results]\n\n# # --- Calculate BLEU Scores ---\n# # Using SmoothingFunction() to handle cases where there are no common n-grams (prevents zero division)\n# smooth = SmoothingFunction().method1\n\n# # Few-shot BLEU\n# bleu_few_shot_scores = []\n# for ref, hyp in zip(few_shot_references, few_shot_hypotheses):\n#     # BLEU expects a list of reference sentences (even if only one)\n#     bleu_few_shot_scores.append(sentence_bleu([ref], hyp, smoothing_function=smooth))\n# avg_bleu_few_shot = sum(bleu_few_shot_scores) / len(bleu_few_shot_scores) if bleu_few_shot_scores else 0\n\n# # Zero-shot BLEU\n# bleu_zero_shot_scores = []\n# for ref, hyp in zip(zero_shot_references, zero_shot_hypotheses):\n#     bleu_zero_shot_scores.append(sentence_bleu([ref], hyp, smoothing_function=smooth))\n# avg_bleu_zero_shot = sum(bleu_zero_shot_scores) / len(bleu_zero_shot_scores) if bleu_zero_shot_scores else 0\n\n# print(f\"Average BLEU Score (Few-Shot): {avg_bleu_few_shot:.4f}\")\n# print(f\"Average BLEU Score (Zero-Shot): {avg_bleu_zero_shot:.4f}\")\n\n# # --- Calculate ROUGE Scores ---\n# rouge_few_shot_scores = {'rouge1': {'fmeasure': 0, 'precision': 0, 'recall': 0},\n#                          'rouge2': {'fmeasure': 0, 'precision': 0, 'recall': 0},\n#                          'rougeL': {'fmeasure': 0, 'precision': 0, 'recall': 0}}\n# count_few_shot = 0\n\n# for item in few_shot_results:\n#     reference = item['ground_truth_summary']\n#     hypothesis = item['instructblip_summary']\n#     if reference and hypothesis: # Ensure both are not empty\n#         scores = scorer.score(reference, hypothesis)\n#         for key in rouge_few_shot_scores:\n#             rouge_few_shot_scores[key]['fmeasure'] += scores[key].fmeasure\n#             rouge_few_shot_scores[key]['precision'] += scores[key].precision\n#             rouge_few_shot_scores[key]['recall'] += scores[key].recall\n#         count_few_shot += 1\n\n# if count_few_shot > 0:\n#     for key in rouge_few_shot_scores:\n#         rouge_few_shot_scores[key]['fmeasure'] /= count_few_shot\n#         rouge_few_shot_scores[key]['precision'] /= count_few_shot\n#         rouge_few_shot_scores[key]['recall'] /= count_few_shot\n\n# rouge_zero_shot_scores = {'rouge1': {'fmeasure': 0, 'precision': 0, 'recall': 0},\n#                           'rouge2': {'fmeasure': 0, 'precision': 0, 'recall': 0},\n#                           'rougeL': {'fmeasure': 0, 'precision': 0, 'recall': 0}}\n# count_zero_shot = 0\n\n# for item in zero_shot_results:\n#     reference = item['ground_truth_summary']\n#     hypothesis = item['instructblip_summary']\n#     if reference and hypothesis: # Ensure both are not empty\n#         scores = scorer.score(reference, hypothesis)\n#         for key in rouge_zero_shot_scores:\n#             rouge_zero_shot_scores[key]['fmeasure'] += scores[key].fmeasure\n#             rouge_zero_shot_scores[key]['precision'] += scores[key].precision\n#             rouge_zero_shot_scores[key]['recall'] += scores[key].recall\n#         count_zero_shot += 1\n\n# if count_zero_shot > 0:\n#     for key in rouge_zero_shot_scores:\n#         rouge_zero_shot_scores[key]['fmeasure'] /= count_zero_shot\n#         rouge_zero_shot_scores[key]['precision'] /= count_zero_shot\n#         rouge_zero_shot_scores[key]['recall'] /= count_zero_shot\n\n# print(\"\\nAverage ROUGE Scores (Few-Shot):\")\n# for key, scores in rouge_few_shot_scores.items():\n#     print(f\"  {key}: F1={scores['fmeasure']:.4f}, P={scores['precision']:.4f}, R={scores['recall']:.4f}\")\n\n# print(\"\\nAverage ROUGE Scores (Zero-Shot):\")\n# for key, scores in rouge_zero_shot_scores.items():\n#     print(f\"  {key}: F1={scores['fmeasure']:.4f}, P={scores['precision']:.4f}, R={scores['recall']:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-27T17:20:22.332Z"}},"outputs":[],"execution_count":null}]}